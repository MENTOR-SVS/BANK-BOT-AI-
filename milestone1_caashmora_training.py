"""
ğŸ¦ Milestone 1 â€” Intent & Entity Recognition Engine
-----------------------------------------------------------
Features:
1ï¸âƒ£ Safely loads multiple possible dataset files
2ï¸âƒ£ Handles CSV quoting, bad lines, and renames
3ï¸âƒ£ Builds & trains TF-IDF + Logistic Regression classifier
4ï¸âƒ£ Extracts key banking entities (slots)
5ï¸âƒ£ Evaluates accuracy and saves all artifacts
6ï¸âƒ£ Tests predictions interactively
-----------------------------------------------------------
Outputs:
 â€¢ models/intent_pipeline.joblib
 â€¢ models/intent_responses.json
 â€¢ models/metrics.json
"""

import os
import json
import re
import joblib
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report

# ----------------------------------------------------------
# ğŸ”§ Configuration
# ----------------------------------------------------------
MODEL_DIR = "models"
MODEL_PATH = os.path.join(MODEL_DIR, "intent_pipeline.joblib")
RESPONSES_PATH = os.path.join(MODEL_DIR, "intent_responses.json")
METRICS_PATH = os.path.join(MODEL_DIR, "metrics.json")

CANDIDATE_FILES = [
    "bankbot_finial_expanded.csv",  # Corrected typo in filename
    "bankbot_final_expanded_v2.csv",
    "bankbot_final_expanded.csv",
    "bank_chatbot_dataset_large.csv",
    "bank_chatbot_dataset_large (2).csv",
    "training_data.csv"
]

# ----------------------------------------------------------
# ğŸ“¥ 1ï¸âƒ£ Load Dataset Safely
# ----------------------------------------------------------
def load_dataset():
    df, loaded_path = None, None
    for candidate in CANDIDATE_FILES:
        if os.path.exists(candidate):
            try:
                df = pd.read_csv(candidate, encoding="utf-8",
                                 on_bad_lines="skip", quotechar='"', escapechar='\\')
                loaded_path = candidate
                break
            except Exception:
                try:
                    df = pd.read_csv(candidate, encoding="utf-8", on_bad_lines="skip")
                    loaded_path = candidate
                    break
                except Exception:
                    pass

    if df is None:
        raise SystemExit(f"âŒ No valid dataset found. Tried: {CANDIDATE_FILES}")

    # Clean column names
    df.columns = [c.strip().lower() for c in df.columns]
    
    # Map common column name variations
    if "text" not in df.columns:
        if "query" in df.columns:
            df["text"] = df["query"]
        elif "question" in df.columns:
            df["text"] = df["question"]
    
    if "response" not in df.columns and "answer" in df.columns:
        df["response"] = df["answer"]
    elif "response" not in df.columns:
        df["response"] = ""

    if not all(col in df.columns for col in ["text", "intent"]):
        print("âŒ Available columns:", list(df.columns))
        raise ValueError("Dataset must contain columns mapped to ['text','intent']")

    df = df.dropna(subset=["text", "intent"]).reset_index(drop=True)
    df["intent"] = df["intent"].astype(str).str.strip().str.lower()
    print(f"âœ… Loaded {loaded_path} â€” {len(df)} rows, {df['intent'].nunique()} intents")
    return df

# ----------------------------------------------------------
# ğŸ§  2ï¸âƒ£ Build Model Pipeline
# ----------------------------------------------------------
def build_pipeline():
    return Pipeline([
        ("tfidf", TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_df=0.95)),
        ("clf", LogisticRegression(max_iter=2000))
    ])

# ----------------------------------------------------------
# ğŸ’¬ 3ï¸âƒ£ Build Intent â†’ Response Map
# ----------------------------------------------------------
def make_intent_response_map(df):
    resp_map = {}
    if "response" in df.columns:
        for _, r in df.iterrows():
            intent = str(r.get("intent", "")).strip().lower()
            resp = str(r.get("response", "")).strip()
            if intent and resp:
                resp_map.setdefault(intent, []).append(resp)
    return resp_map

# ----------------------------------------------------------
# ğŸ§© 4ï¸âƒ£ Entity Extraction (Slot Filling)
# ----------------------------------------------------------
SLOT_PATTERNS = {
    "account_number": r"\b\d{6,16}\b",
    "amount": r"\b\d{2,8}\b",
    "mobile_number": r"\b\d{10}\b",
    "city_name": r"\b(?:chennai|puducherry|mumbai|delhi|bangalore)\b",
    "payment_method": r"\b(?:upi|bank transfer|neft|imps|rtgs)\b",
    "card_type": r"\b(?:debit|credit)\b",
    "account_type": r"\b(?:savings|current)\b"
}

def extract_entities(text):
    entities = {}
    lower = text.lower()
    for slot, pattern in SLOT_PATTERNS.items():
        m = re.search(pattern, lower)
        if m:
            entities[slot] = m.group()
    return entities

# ----------------------------------------------------------
# ğŸš€ 5ï¸âƒ£ Train, Evaluate, Save
# ----------------------------------------------------------
def train_and_save(df):
    X, y = df["text"], df["intent"]
    stratify = y if y.value_counts().min() >= 2 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )

    model = build_pipeline()
    print("ğŸ§  Training model...")
    model.fit(X_train, y_train)

    print("ğŸ” Evaluating model...")
    y_pred = model.predict(X_test)
    report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)
    print(classification_report(y_test, y_pred, zero_division=0))

    os.makedirs(MODEL_DIR, exist_ok=True)
    joblib.dump(model, MODEL_PATH)
    print(f"ğŸ’¾ Model saved â†’ {MODEL_PATH}")

    resp_map = make_intent_response_map(df)
    with open(RESPONSES_PATH, "w", encoding="utf-8") as f:
        json.dump(resp_map, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¬ Responses saved â†’ {RESPONSES_PATH}")

    metrics = {
        "n_rows": len(df),
        "n_intents": df["intent"].nunique(),
        "report": report
    }
    with open(METRICS_PATH, "w", encoding="utf-8") as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“Š Metrics saved â†’ {METRICS_PATH}")

    return model

# ----------------------------------------------------------
# ğŸ§ª 6ï¸âƒ£ Test Predictions
# ----------------------------------------------------------
def test_sample(model, query):
    pred = model.predict([query])[0]
    prob = max(model.predict_proba([query])[0])
    ents = extract_entities(query)
    print(f"\nğŸ—£ {query}")
    print(f"ğŸ¯ Intent: {pred} (Confidence: {prob:.2f})")
    print(f"ğŸ“ Entities: {ents}")

# ----------------------------------------------------------
# â–¶ï¸ Main Entry
# ----------------------------------------------------------
def main():
    df = load_dataset()
    model = train_and_save(df)

    print("\nâœ… Testing few sample queries...")
    samples = [
        "Check my account balance",
        "Transfer 5000 to account 9876543210 via UPI",
        "Block my debit card",
        "Increase my credit card limit",
        "Nearest ATM in Puducherry",
        "Apply for a new savings account"
    ]
    for s in samples:
        test_sample(model, s)

    print("\nğŸ¯ Milestone 1 completed successfully â€” model ready for Milestone 2.")

if __name__ == "__main__":
    main()

